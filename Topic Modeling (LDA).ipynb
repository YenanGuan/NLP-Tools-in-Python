{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "import pyLDAvis.gensim\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/thinc/neural/train.py:7: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from .optimizers import Adam, linear_decay\n",
      "/usr/local/lib/python3.7/site-packages/thinc/check.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence, Sized, Iterable, Callable\n",
      "/usr/local/lib/python3.7/site-packages/thinc/check.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence, Sized, Iterable, Callable\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "\n",
    "parser = English()\n",
    "\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_of_speech(list_input):\n",
    "    # nltk.download('averaged_perceptron_tagger')\n",
    "    word_list = []\n",
    "    for i in range(len(list_input)):\n",
    "        word_list.append(list_input[i])\n",
    "        word_list = ' '.join(word_list)\n",
    "        word_list = nltk.word_tokenize(word_list)\n",
    "        word_tagged = nltk.pos_tag(word_list)\n",
    "        pos_list = ['VB', 'VBZ', 'VBN', 'VBG', 'VBD', 'NN', 'NNS', 'JJ', 'RB'] #verbs, nouns, adj, adv\n",
    "        pos_selected_words = []\n",
    "        for i, j in word_tagged:\n",
    "            if j in pos_list:\n",
    "                pos_selected_words.append(i)\n",
    "    return pos_selected_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Text for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional stopwords\n",
    "en_stop = list(nltk.corpus.stopwords.words('english'))\n",
    "for i in ['would like', 'would', 'when', 'with']:\n",
    "    en_stop.append(i)\n",
    "\n",
    "def prepare_by_line(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 3]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    if len(tokens)>1:\n",
    "        temp_pos = part_of_speech(tokens)\n",
    "        tokens = [token for token in temp_pos]\n",
    "    return tokens\n",
    "\n",
    "def prepare_text_for_lda(df_column):  \n",
    "    text_data = []\n",
    "    discard_index = []\n",
    "    discard_content = []\n",
    "    for i in range(len(df_column)):\n",
    "        line = df_column.iloc[i][0]\n",
    "        tokens = prepare_by_line(line)\n",
    "        if len(tokens)>1:\n",
    "            text_data.append(tokens)\n",
    "        else:\n",
    "            discard_index.append(i)\n",
    "            discard_content.append(line)\n",
    "            print(i, '  ', line)\n",
    "    return text_data, discard_index, discard_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_fit(text_data, num_topics):\n",
    "    # Create Dictionary & Corpus\n",
    "    dictionary = corpora.Dictionary(text_data)\n",
    "    \n",
    "#     count = 0\n",
    "#     for k, v in dictionary.iteritems():\n",
    "#         print(k, v)\n",
    "#         count += 1\n",
    "#         if count > 10:\n",
    "#             break\n",
    "    \n",
    "    dictionary.filter_extremes(no_below = 3, no_above=0.5)\n",
    "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "    pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "    dictionary.save('dictionary.gensim')\n",
    "    \n",
    "    # Fit LDA\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = num_topics, id2word=dictionary, passes=20, \n",
    "                                               random_state=234)\n",
    "    ldamodel.save('model.gensim')\n",
    "    topics = ldamodel.print_topics(num_words=5)\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "    \n",
    "    # LDA Visualization\n",
    "    dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "    corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
    "    lda = gensim.models.ldamodel.LdaModel.load('model.gensim')\n",
    "    \n",
    "    lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "    # pyLDAvis.save_html(lda_display, 'topic-'+which_column.split()[-1]+'.html')\n",
    "    # pyLDAvis.display(lda_display)\n",
    "    return text_data, corpus, ldamodel, lda_display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Dataframe with Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_topic(model, corpus, df, filter_id, filter_topic_value):\n",
    "    topic_assigned = []\n",
    "    for i, row_list in enumerate(model[corpus]):\n",
    "        which_topic = np.argmax(row_list, axis=0)[1]\n",
    "        topic_assigned.append(which_topic)\n",
    "        \n",
    "    df_temp = df.iloc[filter_id]\n",
    "    df_output = df.copy()\n",
    "    df_output.drop(df.index[[filter_id]], inplace=True)    \n",
    "    df_output['topic'] = topic_assigned\n",
    "    df_temp['topic'] = filter_topic_value\n",
    "    df_output = pd.concat([df_output, df_temp])\n",
    "    df_output.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data and Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/defusedxml/ElementTree.py:68: DeprecationWarning: The html argument of XMLParser() is deprecated\n",
      "  _XMLParser.__init__(self, html, target, encoding)\n",
      "/usr/local/lib/python3.7/site-packages/defusedxml/ElementTree.py:68: DeprecationWarning: The html argument of XMLParser() is deprecated\n",
      "  _XMLParser.__init__(self, html, target, encoding)\n",
      "/usr/local/lib/python3.7/site-packages/defusedxml/ElementTree.py:68: DeprecationWarning: The html argument of XMLParser() is deprecated\n",
      "  _XMLParser.__init__(self, html, target, encoding)\n",
      "/usr/local/lib/python3.7/site-packages/defusedxml/ElementTree.py:68: DeprecationWarning: The html argument of XMLParser() is deprecated\n",
      "  _XMLParser.__init__(self, html, target, encoding)\n",
      "/usr/local/lib/python3.7/site-packages/defusedxml/ElementTree.py:68: DeprecationWarning: The html argument of XMLParser() is deprecated\n",
      "  _XMLParser.__init__(self, html, target, encoding)\n",
      "/usr/local/lib/python3.7/site-packages/defusedxml/ElementTree.py:68: DeprecationWarning: The html argument of XMLParser() is deprecated\n",
      "  _XMLParser.__init__(self, html, target, encoding)\n",
      "/usr/local/lib/python3.7/site-packages/defusedxml/ElementTree.py:68: DeprecationWarning: The html argument of XMLParser() is deprecated\n",
      "  _XMLParser.__init__(self, html, target, encoding)\n",
      "/usr/local/lib/python3.7/site-packages/defusedxml/ElementTree.py:68: DeprecationWarning: The html argument of XMLParser() is deprecated\n",
      "  _XMLParser.__init__(self, html, target, encoding)\n",
      "/usr/local/lib/python3.7/site-packages/defusedxml/ElementTree.py:68: DeprecationWarning: The html argument of XMLParser() is deprecated\n",
      "  _XMLParser.__init__(self, html, target, encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    none\n",
      "7    Didn't have a bad odor\n",
      "20    no dust\n",
      "32    I like the scent\n",
      "34    No odors\n",
      "36    everything\n",
      "45    Everything\n",
      "47    i liked everything\n",
      "50    consistency\n",
      "58    It was clumping\n",
      "64    LIGHT\n",
      "86    low dust\n",
      "90    No it is new\n",
      "96    was lightweight\n",
      "(0, '0.127*\"easy\" + 0.096*\"scoop\" + 0.080*\"litter\" + 0.057*\"pour\" + 0.053*\"odor\"')\n",
      "(1, '0.159*\"smell\" + 0.096*\"scent\" + 0.062*\"clean\" + 0.058*\"litter\" + 0.051*\"easy\"')\n",
      "(2, '0.194*\"clump\" + 0.150*\"well\" + 0.135*\"dust\" + 0.076*\"great\" + 0.068*\"really\"')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diana_evolve/Library/Python/3.7/lib/python/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4    Nothing\n",
      "7    Nothing\n",
      "12    Nothing\n",
      "17    no\n",
      "20    nothing\n",
      "21    nothing\n",
      "28    Nothing I didn’t like.\n",
      "29    Na\n",
      "31    None.\n",
      "32    none\n",
      "33    Nothing\n",
      "34    Nothing\n",
      "35    nothing\n",
      "39    none\n",
      "41    Tracked out of box\n",
      "43    nothing\n",
      "44    None\n",
      "46    nothing\n",
      "48    nothing\n",
      "49    none\n",
      "51    nothing\n",
      "52    none\n",
      "53    none\n",
      "55    a bit heavy\n",
      "61    Smell\n",
      "62    didn't like the scent\n",
      "65    nothing at all\n",
      "68    Nothing\n",
      "69    nothing\n",
      "75    Nothing\n",
      "81    Nothing\n",
      "86    Nothing\n",
      "90    Nothing\n",
      "(0, '0.090*\"clump\" + 0.083*\"stick\" + 0.065*\"litter\" + 0.063*\"track\" + 0.063*\"also\"')\n",
      "(1, '0.153*\"clump\" + 0.108*\"dislike\" + 0.077*\"well\" + 0.066*\"nothing\" + 0.066*\"urine\"')\n",
      "(2, '0.227*\"scent\" + 0.109*\"strong\" + 0.107*\"litter\" + 0.107*\"really\" + 0.074*\"little\"')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diana_evolve/Library/Python/3.7/lib/python/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4    Nothing\n",
      "7    No improvement\n",
      "9    hold in odors better\n",
      "12    Nothing\n",
      "19    A bit firmwr and not as lose.\n",
      "20    none\n",
      "21    none\n",
      "29    Perfect as is.\n",
      "31    not sure\n",
      "34    None\n",
      "40    nothing at all\n",
      "44    nothing\n",
      "45    None\n",
      "47    nothing\n",
      "49    none\n",
      "50    none\n",
      "52    no idea\n",
      "54    unsure\n",
      "55    clumping\n",
      "62    I didn’t like the smell.\n",
      "63    none\n",
      "69    Nothing\n",
      "70    nothing\n",
      "72    Now ne\n",
      "74    More fragrance\n",
      "79    would like more scent\n",
      "81    Clump better\n",
      "87    Na\n",
      "90    I would like to be more clumpy\n",
      "93    none\n",
      "95    not sure\n",
      "(0, '0.160*\"make\" + 0.093*\"clump\" + 0.067*\"little\" + 0.066*\"fine\" + 0.052*\"track\"')\n",
      "(1, '0.235*\"scent\" + 0.100*\"smell\" + 0.086*\"fragrance\" + 0.084*\"think\" + 0.059*\"litter\"')\n",
      "(2, '0.161*\"le\" + 0.086*\"need\" + 0.086*\"improvement\" + 0.073*\"scoop\" + 0.060*\"clumping\"')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diana_evolve/Library/Python/3.7/lib/python/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('Copy of Verbatims.xlsx')\n",
    "\n",
    "num_topics = 3\n",
    "\n",
    "which_column = 'Likes'\n",
    "df_like = df.loc[:, [which_column]].dropna()\n",
    "text_data, id_like, content_like = prepare_text_for_lda(df_like)\n",
    "text_like, corpus_like, model_like, display_like = LDA_fit(text_data, num_topics)\n",
    "\n",
    "which_column = 'Dislikes'\n",
    "df_dislike = df.loc[:, [which_column]].dropna()\n",
    "text_data, id_dislike, content_dislike = prepare_text_for_lda(df_dislike)\n",
    "text_dislike, corpus_dislike, model_dislike, display_dislike = LDA_fit(text_data, num_topics)\n",
    "\n",
    "which_column = 'Improvements'\n",
    "df_imp = df.loc[:, [which_column]].dropna()\n",
    "text_data, id_imp, content_imp = prepare_text_for_lda(df_imp)\n",
    "text_imp, corpus_imp, model_imp, display_imp = LDA_fit(text_data, num_topics)\n",
    "\n",
    "# create and save visulized html file\n",
    "pyLDAvis.save_html(display_like, 'topic-'+which_column.split()[-1]+'.html')\n",
    "pyLDAvis.save_html(display_dislike, 'topic-'+which_column.split()[-1]+'.html')\n",
    "pyLDAvis.save_html(display_imp, 'topic-'+which_column.split()[-1]+'.html')\n",
    "\n",
    "# pyLDAvis.display(display_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pandas/core/indexes/base.py:3969: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  result = getitem(key)\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "final_like = combine_topic(model_like, corpus_like, df_like, id_like, 'None-Like')\n",
    "final_dislike = combine_topic(model_dislike, corpus_dislike, df_dislike, id_dislike, 'None-Disike')\n",
    "final_imp = combine_topic(model_imp, corpus_imp, df_imp, id_imp, 'None-Imp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('Topic Modelling-Result.xlsx') as writer:\n",
    "    final_like.to_excel(writer, sheet_name='like', index=False)\n",
    "    final_dislike.to_excel(writer, sheet_name='dislike', index=False)\n",
    "    final_imp.to_excel(writer, sheet_name='imp', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[('easy', 0.12668732), ('scoop', 0.09605881), ('litter', 0.079764895), ('pour', 0.056900606), ('odor', 0.05289848)]\n",
      "1\n",
      "[('smell', 0.15874365), ('scent', 0.09579791), ('clean', 0.062003337), ('litter', 0.058434166), ('easy', 0.050696924)]\n",
      "2\n",
      "[('clump', 0.19439168), ('well', 0.15011688), ('dust', 0.13455021), ('great', 0.07555453), ('really', 0.06760639)]\n"
     ]
    }
   ],
   "source": [
    "# for i,topic in model_like.show_topics(formatted=False, num_topics=5, num_words=5):\n",
    "#     a = topic\n",
    "#     print(str(i))\n",
    "#     print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[('clump', 0.08990982), ('stick', 0.08333188), ('litter', 0.06454768), ('track', 0.06339767), ('also', 0.06290527)]\n",
      "1\n",
      "[('clump', 0.1527611), ('dislike', 0.107528806), ('well', 0.077133484), ('nothing', 0.06595048), ('urine', 0.06581002)]\n",
      "2\n",
      "[('scent', 0.22685793), ('strong', 0.108979136), ('litter', 0.107217245), ('really', 0.10696874), ('little', 0.0742732)]\n"
     ]
    }
   ],
   "source": [
    "# for i,topic in model_dislike.show_topics(formatted=False, num_topics=5, num_words=5):\n",
    "#     a = topic\n",
    "#     print(str(i))\n",
    "#     print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[('make', 0.15967935), ('clump', 0.09318521), ('little', 0.06679565), ('fine', 0.06614576), ('track', 0.05159173)]\n",
      "1\n",
      "[('scent', 0.23459512), ('smell', 0.09985224), ('fragrance', 0.085822575), ('think', 0.08398967), ('litter', 0.058729425)]\n",
      "2\n",
      "[('le', 0.16142654), ('need', 0.08629921), ('improvement', 0.086147375), ('scoop', 0.07265034), ('clumping', 0.05960957)]\n"
     ]
    }
   ],
   "source": [
    "# for i,topic in model_imp.show_topics(formatted=False, num_topics=5, num_words=5):\n",
    "#     a = topic\n",
    "#     print(str(i))\n",
    "#     print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
