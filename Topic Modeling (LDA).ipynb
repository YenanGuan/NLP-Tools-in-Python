{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "import pyLDAvis.gensim\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "\n",
    "parser = English()\n",
    "\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_of_speech(list_input):\n",
    "    # nltk.download('averaged_perceptron_tagger')\n",
    "    word_list = []\n",
    "    for i in range(len(list_input)):\n",
    "        word_list.append(list_input[i])\n",
    "        word_list = ' '.join(word_list)\n",
    "        word_list = nltk.word_tokenize(word_list)\n",
    "        word_tagged = nltk.pos_tag(word_list)\n",
    "        pos_list = ['VB', 'VBZ', 'VBN', 'VBG', 'VBD', 'NN', 'NNS', 'JJ', 'RB'] #verbs, nouns, adj, adv\n",
    "        pos_selected_words = []\n",
    "        for i, j in word_tagged:\n",
    "            if j in pos_list:\n",
    "                pos_selected_words.append(i)\n",
    "    return pos_selected_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Text for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional stopwords\n",
    "en_stop = list(nltk.corpus.stopwords.words('english'))\n",
    "for i in ['would like', 'would', 'when', 'with']:\n",
    "    en_stop.append(i)\n",
    "\n",
    "def prepare_by_line(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 3]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    if len(tokens)>1:\n",
    "        temp_pos = part_of_speech(tokens)\n",
    "        tokens = [token for token in temp_pos]\n",
    "    return tokens\n",
    "\n",
    "def prepare_text_for_lda(df_column):  \n",
    "    text_data = []\n",
    "    discard_index = []\n",
    "    discard_content = []\n",
    "    for i in range(len(df_column)):\n",
    "        line = df_column.iloc[i][0]\n",
    "        tokens = prepare_by_line(line)\n",
    "        if len(tokens)>1:\n",
    "            text_data.append(tokens)\n",
    "        else:\n",
    "            discard_index.append(i)\n",
    "            discard_content.append(line)\n",
    "            # print the rows that are left out from model data preparation \n",
    "            print(i, '  ', line)\n",
    "    return text_data, discard_index, discard_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_fit(text_data, num_topics):\n",
    "    # Create Dictionary & Corpus\n",
    "    dictionary = corpora.Dictionary(text_data)\n",
    "    \n",
    "#     count = 0\n",
    "#     for k, v in dictionary.iteritems():\n",
    "#         print(k, v)\n",
    "#         count += 1\n",
    "#         if count > 10:\n",
    "#             break\n",
    "    \n",
    "    dictionary.filter_extremes(no_below = 3, no_above=0.5)\n",
    "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "    pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "    dictionary.save('dictionary.gensim')\n",
    "    \n",
    "    # Fit LDA\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = num_topics, id2word=dictionary, passes=20, random_state=234)\n",
    "    ldamodel.save('model.gensim')\n",
    "    topics = ldamodel.print_topics(num_words=5)\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "    \n",
    "    # LDA Visualization\n",
    "    dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "    corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
    "    lda = gensim.models.ldamodel.LdaModel.load('model.gensim')\n",
    "    \n",
    "    lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "    # pyLDAvis.save_html(lda_display, 'topic-'+which_column.split()[-1]+'.html')\n",
    "    # pyLDAvis.display(lda_display)\n",
    "    return text_data, corpus, ldamodel, lda_display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Dataframe with Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_topic(model, corpus, df, filter_id, filter_topic_value):\n",
    "    topic_assigned = []\n",
    "    for i, row_list in enumerate(model[corpus]):\n",
    "        which_topic = np.argmax(row_list, axis=0)[1]\n",
    "        topic_assigned.append(which_topic)\n",
    "        \n",
    "    df_temp = df.iloc[filter_id]\n",
    "    df_output = df.copy()\n",
    "    df_output.drop(df.index[[filter_id]], inplace=True)    \n",
    "    df_output['topic'] = topic_assigned\n",
    "    df_temp['topic'] = filter_topic_value\n",
    "    df_output = pd.concat([df_output, df_temp])\n",
    "    df_output.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data and Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Copy of Verbatims.xlsx')\n",
    "\n",
    "num_topics = 3\n",
    "\n",
    "which_column = 'Likes'\n",
    "df_like = df.loc[:, [which_column]].dropna()\n",
    "text_data, id_like, content_like = prepare_text_for_lda(df_like)\n",
    "text_like, corpus_like, model_like, display_like = LDA_fit(text_data, num_topics)\n",
    "\n",
    "which_column = 'Dislikes'\n",
    "df_dislike = df.loc[:, [which_column]].dropna()\n",
    "text_data, id_dislike, content_dislike = prepare_text_for_lda(df_dislike)\n",
    "text_dislike, corpus_dislike, model_dislike, display_dislike = LDA_fit(text_data, num_topics)\n",
    "\n",
    "which_column = 'Improvements'\n",
    "df_imp = df.loc[:, [which_column]].dropna()\n",
    "text_data, id_imp, content_imp = prepare_text_for_lda(df_imp)\n",
    "text_imp, corpus_imp, model_imp, display_imp = LDA_fit(text_data, num_topics)\n",
    "\n",
    "# create and save visulized html file\n",
    "pyLDAvis.save_html(display_like, 'topic-'+which_column.split()[-1]+'.html')\n",
    "pyLDAvis.save_html(display_dislike, 'topic-'+which_column.split()[-1]+'.html')\n",
    "pyLDAvis.save_html(display_imp, 'topic-'+which_column.split()[-1]+'.html')\n",
    "\n",
    "# pyLDAvis.display(display_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_like = combine_topic(model_like, corpus_like, df_like, id_like, 'None-Like')\n",
    "final_dislike = combine_topic(model_dislike, corpus_dislike, df_dislike, id_dislike, 'None-Disike')\n",
    "final_imp = combine_topic(model_imp, corpus_imp, df_imp, id_imp, 'None-Imp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('Topic Modelling-Result.xlsx') as writer:\n",
    "    final_like.to_excel(writer, sheet_name='like', index=False)\n",
    "    final_dislike.to_excel(writer, sheet_name='dislike', index=False)\n",
    "    final_imp.to_excel(writer, sheet_name='imp', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,topic in model_like.show_topics(formatted=False, num_topics=5, num_words=5):\n",
    "    a = topic\n",
    "    print(str(i))\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,topic in model_dislike.show_topics(formatted=False, num_topics=5, num_words=5):\n",
    "    a = topic\n",
    "    print(str(i))\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,topic in model_imp.show_topics(formatted=False, num_topics=5, num_words=5):\n",
    "    a = topic\n",
    "    print(str(i))\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
